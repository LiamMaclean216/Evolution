{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "envs = ['CartPole-v1','Acrobot-v1','MountainCar-v0','Pendulum-v0','BipedalWalker-v2']\n",
    "env = gym.make(envs[0]).unwrapped\n",
    "\n",
    "discrete_actions = True\n",
    "#TODO\n",
    "#parralel fitness measuring\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "class Creature(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Creature, self).__init__()\n",
    "    \n",
    "        self.layer1 = nn.Linear(env.observation_space.shape[0], 6)\n",
    "        self.layer2 = nn.Linear(6, 6)\n",
    "        \n",
    "        if discrete_actions:\n",
    "            self.layer3 = nn.Linear(6, env.action_space.n)\n",
    "        else:\n",
    "            self.layer3 = nn.Linear(6, env.action_space.shape[0])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,output_num):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5, stride=1, padding=0),  \n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(16, 8, 5, stride=1, padding=0),  \n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(2, stride=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(8, 32, 5, stride=1,padding=0),  \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(32, 16, 5, stride=1,padding=0),  \n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(2, stride=2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, 5, stride=1,padding=0),  \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(32, 16, 5, stride=1,padding=0),  \n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(2, stride=2))\n",
    "        \n",
    "        self.layer4 =  nn.Linear(16*14, output_num)\n",
    "    def forward(self, out):\n",
    "        out = out.unsqueeze(1)\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        \n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0),out.size(1)*out.size(2))\n",
    "        \n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5, stride=1, padding=0),  \n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(16, 8, 5, stride=1, padding=0),  \n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool1d(2, stride=1))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(8, 32, 5, stride=1,padding=0),  \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(32, 16, 5, stride=1,padding=0),  \n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool1d(2, stride=1))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, 5, stride=1,padding=0),  \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(32, 16, 5, stride=1,padding=0),  \n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool1d(2, stride=1))\n",
    "        \n",
    "        self.layer4 = nn.Linear(16*59, 128)\n",
    "        self.layer5 = nn.Sequential(       \n",
    "            nn.Linear(128, 2),\n",
    "            nn.Sigmoid())\n",
    "    def forward(self, out):\n",
    "        out = out.unsqueeze(1)\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0),out.size(1)*out.size(2))\n",
    "        \n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        return out\n",
    "#gen = Generator(86).to(device)\n",
    "\n",
    "#gen(torch.zeros([10,86*2]).to(device)).shape\n",
    "\n",
    "#dis = Discriminator().to(device)\n",
    "#dis(torch.zeros([10,86]).to(device)).shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mate(m,d,gen,apply_mutation = True,dominance = 0.5,mutation_rate=0.2):\n",
    "    dom = torch.from_numpy(np.array(dominance)).to(device).unsqueeze(-1).type(\"torch.cuda.FloatTensor\")\n",
    "    child = Creature()\n",
    "    mom = (m)\n",
    "    dad = (d)\n",
    "    if apply_mutation:\n",
    "        mom = mutate(mom,mutation_rate=mutation_rate)\n",
    "        dad = mutate(dad,mutation_rate=mutation_rate)\n",
    "    mom = get_params(mom)\n",
    "    dad = get_params(dad)\n",
    "    generated = gen(torch.cat([dad,mom,dom]).unsqueeze(0)).squeeze(0)\n",
    "    child = set_params(child,generated)\n",
    "    if apply_mutation:\n",
    "        child = mutate(child,mutation_rate=mutation_rate)\n",
    "    return child\n",
    "\n",
    "def mutate(creature,mutation_rate=0.2):\n",
    "    new = Creature().to(device)\n",
    "    new.load_state_dict(creature.state_dict()) \n",
    "    for p in new.parameters():\n",
    "\n",
    "        mutation = np.random.normal(scale = 0.07,size = p.data.shape)\n",
    "        mutation *= np.random.choice([1, 0], p.data.shape,p=[mutation_rate,1-mutation_rate])\n",
    "        mutation = torch.from_numpy(mutation).type('torch.FloatTensor').to(device)\n",
    "        p.data += mutation\n",
    "    return new\n",
    "\n",
    "def evolve(population,gen,pf_fitness,mutate):\n",
    "    p_fitness_positive = p_fitness - np.min(p_fitness) + 1\n",
    "    pick_probabilities = get_pick_probabilities(pf_fitness)\n",
    "    \n",
    "    \n",
    "    choice = np.random.choice(pick_probabilities.size,population_size, p = pick_probabilities)\n",
    "    new_population = []\n",
    "    \n",
    "    for p in range(len(population)-1):\n",
    "        first_choice = population[choice[p]]\n",
    "        second_choice = population[choice[p+1]]\n",
    "        #more succesful(healthier?) creature has greater genetic dominance\n",
    "        f1 = p_fitness_positive[p]\n",
    "        f2 = p_fitness_positive[p+1]\n",
    "        if  f1>=f2 :\n",
    "            dominance = (f2/f1) * 0.5\n",
    "            np.clip(dominance,0.3,0.7)\n",
    "            child = mate(second_choice,first_choice,gen, mutate,dominance).to(device)\n",
    "        else:\n",
    "            dominance = (f1/f2) * 0.5\n",
    "            np.clip(dominance,0.3,0.7)\n",
    "            child = mate(first_choice,second_choice,gen, mutate,dominance).to(device)\n",
    "            \n",
    "        new_population.append(child)\n",
    "        \n",
    "    child = mate(population[0],population[len(population)-1],gen, mutate).to(device) \n",
    "    new_population.append(child)\n",
    "    \n",
    "    return new_population\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_gan(population,p_fitness,old_fitness,batch_size = 20,n_epochs = 100):\n",
    "    \n",
    "    \n",
    "    cat = np.concatenate([p_fitness,old_fitness])\n",
    "    #min_fit = np.sort(cat)[int(cat.size*0.75)]\n",
    "    min_fit = np.mean(np.sort(cat)[int(cat.size)-int(cat.size/2):])\n",
    "    #min_fit2 = np.mean(np.sort(p_fitness)[int(p_fitness.size)-int(p_fitness.size/2):])\n",
    "    \n",
    "    ranking = (p_fitness>=min_fit)*1 * (p_fitness/np.max(p_fitness))\n",
    "    \n",
    "    print(min_fit)\n",
    "        \n",
    "    print(ranking)\n",
    "    #n_epochs = n_epochs - (ranking.size - np.count_nonzero(ranking))\n",
    "    #print(n_epochs)\n",
    "    ranking = torch.from_numpy(ranking).to(device).type(\"torch.cuda.FloatTensor\")\n",
    "    gen_error = 0\n",
    "    for e in range(n_epochs):\n",
    "        \n",
    "        for i in range(len(population)//batch_size):\n",
    "\n",
    "            gen_optimizer.zero_grad()\n",
    "            dis_optimizer.zero_grad()\n",
    "            \n",
    "            real_batch = []\n",
    "            for b in range(batch_size):\n",
    "                real_batch.append(get_params(population[(i*batch_size)+b]).unsqueeze(0))\n",
    "            real_batch = torch.cat(real_batch, dim=0).to(device)\n",
    "\n",
    "            if e % 2 == 0:\n",
    "                #train discriminator on population\n",
    "                dis_out = dis(real_batch).squeeze(-1)\n",
    "                #if e == 0 and i == 0:\n",
    "                \n",
    "                    \n",
    "                stack = [ranking[i*batch_size:(i*batch_size)+batch_size],torch.ones(batch_size).to(device)]\n",
    "                stack = torch.stack(stack)\n",
    "                #print(dis_out)\n",
    "                #print(stack.transpose(0,1))\n",
    "                #dis_error_real = nn.BCELoss()(dis_out, ranking[i*batch_size:(i*batch_size)+batch_size])#torch.ones(batch_size).to(device))\n",
    "                dis_error_real = nn.BCELoss()(dis_out,stack.transpose(0,1))\n",
    "                \n",
    "                dis_error_real.backward()\n",
    "                #print(\"Discriminator loss real : {}\".format(dis_error_real))\n",
    "        \n",
    "            #train discriminator on generator output\n",
    "            mom = []\n",
    "            dad = []\n",
    "            child = []\n",
    "            dominance = torch.from_numpy((np.random.rand(batch_size)*0.5) + 0.25).to(device).unsqueeze(-1)\n",
    "            dominance = dominance.type(\"torch.cuda.FloatTensor\")\n",
    "            for b in range(batch_size):\n",
    "                m = get_params(random.choice(population))\n",
    "                d = get_params(random.choice(population))\n",
    "                c_data = torch.cat([m,d,dominance[b]]).unsqueeze(0)\n",
    "                #c_data = torch.cat([c_data,dominance[b]]).unsqueeze(0)\n",
    "                c = gen(c_data).squeeze(0)\n",
    "\n",
    "                mom.append(m)\n",
    "                dad.append(d)\n",
    "                child.append(c)\n",
    "            \n",
    "            mom = torch.stack(mom).to(device)\n",
    "            dad = torch.stack(dad).to(device)\n",
    "            child = torch.stack(child).to(device)\n",
    "            dis_out = dis(child).squeeze(-1)\n",
    "            \n",
    "            if gen_error < 1:\n",
    "                dis_error_fake = nn.BCELoss()(dis_out,torch.zeros(dis_out.shape).to(device)) \n",
    "                dis_error_fake.backward(retain_graph=True)\n",
    "                #print(\"Discriminator loss generated : {}\".format(dis_error_fake))\n",
    "            dis_optimizer.step()        \n",
    "            \n",
    "            #train generator\n",
    "            if e % 1 == 0:\n",
    "                mom_loss = torch.pow(torch.sub(child,mom),2) * (dominance)\n",
    "                dad_loss = torch.pow(torch.sub(child,dad),2) * (1-dominance)\n",
    "\n",
    "                mom_loss = torch.mean(mom_loss)\n",
    "                dad_loss = torch.mean(dad_loss)\n",
    "                if mom_loss > dad_loss:\n",
    "                    child_error = torch.div(mom_loss,dad_loss)-1\n",
    "                else:\n",
    "                    child_error = torch.div(dad_loss,mom_loss)-1\n",
    "\n",
    "\n",
    "                child_error += (mom_loss + dad_loss)\n",
    "\n",
    "                gen_error = nn.BCELoss()(dis_out,torch.ones(dis_out.shape).to(device)) + (child_error*0.1)  \n",
    "                gen_error.backward()\n",
    "                gen_optimizer.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            #print(\"Generator loss : {}\".format(gen_error))\n",
    "            #print(\"Child error : {}\".format(child_error*0.1))\n",
    "            #print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "[ 9. 10. 10. 10.  9. 10. 10. 10. 27. 11. 10. 10. 10.  9.  8.  9.  9. 10.\n",
      "  8. 10.  9.  8. 27. 10.  8.  9.  8.  9. 11.  9. 10. 15.  8. 10. 11. 10.\n",
      " 10. 10.  9.  8.  9. 17.  9. 11.  9. 10.  9. 10.  9.  9.]\n",
      "12.0\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.55555556 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.62962963\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "train\n",
      "Generation 1  fitness : 27.0\n",
      "#################################\n",
      "[ 58. 117.  35. 106.  36.  65.  35.  21.  41.  30.  40.  83.  26.  50.\n",
      "  52.  43. 101.  88.  85.  55.  19.  27.  73. 138.  21.  22.  12.  38.\n",
      "  50. 127.  28.  45.  94.  37. 122.  25.  34.  22. 123.  40. 117.  32.\n",
      "  41.  22.  26.  40.  57.  96. 151.  31.]\n",
      "58.0\n",
      "[0.38410596 0.77483444 0.         0.70198675 0.         0.43046358\n",
      " 0.         0.         0.         0.         0.         0.54966887\n",
      " 0.         0.         0.         0.         0.66887417 0.58278146\n",
      " 0.56291391 0.         0.         0.         0.48344371 0.91390728\n",
      " 0.         0.         0.         0.         0.         0.8410596\n",
      " 0.         0.         0.62251656 0.         0.80794702 0.\n",
      " 0.         0.         0.81456954 0.         0.77483444 0.\n",
      " 0.         0.         0.         0.         0.         0.63576159\n",
      " 1.         0.        ]\n",
      "train\n",
      "Generation 2  fitness : 151.0\n",
      "#################################\n",
      "[30. 34. 27. 29. 39. 31. 45. 40. 31. 33. 28. 29. 25. 26. 26. 29. 30. 29.\n",
      " 23. 28. 23. 33. 24. 31. 25. 25. 28. 29. 28. 31. 35. 24. 26. 33. 32. 32.\n",
      " 28. 20. 28. 20. 30. 42. 35. 24. 35. 23. 31. 33. 24. 24.]\n",
      "60.92\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "train\n",
      "Generation 3  fitness : 45.0\n",
      "#################################\n",
      "[ 81. 103. 140.  74.  61.  63. 112.  50.  62. 105. 135.  82.  49.  85.\n",
      " 120.  42. 111.  83.  80.  70.  63. 106. 117. 105.  43.  81. 104.  84.\n",
      "  44.  94. 121. 112.  67. 105.  50.  83.  68.  90. 121.  58.  48.  35.\n",
      " 111.  56. 110.  74.  77.  71.  61.  56.]\n",
      "82.66\n",
      "[0.         0.73571429 1.         0.         0.         0.\n",
      " 0.8        0.         0.         0.75       0.96428571 0.\n",
      " 0.         0.60714286 0.85714286 0.         0.79285714 0.59285714\n",
      " 0.         0.         0.         0.75714286 0.83571429 0.75\n",
      " 0.         0.         0.74285714 0.6        0.         0.67142857\n",
      " 0.86428571 0.8        0.         0.75       0.         0.59285714\n",
      " 0.         0.64285714 0.86428571 0.         0.         0.\n",
      " 0.79285714 0.         0.78571429 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "train\n",
      "Generation 4  fitness : 140.0\n",
      "#################################\n",
      "[ 76.  45.  68.  90.  68.  60. 121.  52.  98. 148. 104.  56. 158. 134.\n",
      "  46. 129.  74.  94.  44.  54. 117.  95.  74.  73.  62. 137.  94.  60.\n",
      " 114. 140.  73.  50.  62.  44.  90. 108.  60.  88.  54.  96.  66. 113.\n",
      " 125.  95.  64. 119.  72.  78. 153.  82.]\n",
      "109.24\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.76582278 0.         0.         0.93670886 0.         0.\n",
      " 1.         0.84810127 0.         0.8164557  0.         0.\n",
      " 0.         0.         0.74050633 0.         0.         0.\n",
      " 0.         0.86708861 0.         0.         0.72151899 0.88607595\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.71518987\n",
      " 0.79113924 0.         0.         0.75316456 0.         0.\n",
      " 0.96835443 0.        ]\n",
      "train\n",
      "Generation 5  fitness : 158.0\n",
      "#################################\n",
      "[ 37. 107.  45.  97.  60.  54.  55.  42.  46.  95.  38.  81.  39. 121.\n",
      "  86.  38.  80.  58.  80.  37.  37.  95.  42.  81.  86. 103.  85.  94.\n",
      "  46.  44.  78.  40.  52.  38.  55.  54.  32. 104. 131.  83.  47.  55.\n",
      "  63.  30.  42.  61.  44.  70.  79.  61.]\n",
      "101.12\n",
      "[0.         0.81679389 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.92366412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.78625954 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.79389313 1.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "train\n",
      "Generation 6  fitness : 131.0\n",
      "#################################\n",
      "[51. 49. 41. 26. 27. 33. 26. 50. 27. 29. 50. 31. 47. 44. 30. 41. 44. 31.\n",
      " 32. 27. 26. 42. 41. 29. 30. 33. 30. 46. 29. 41. 44. 37. 36. 26. 33. 36.\n",
      " 33. 34. 42. 30. 43. 30. 28. 31. 26. 45. 34. 25. 32. 40.]\n",
      "66.66\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "train\n",
      "Generation 7  fitness : 51.0\n",
      "#################################\n",
      "[ 81.  74.  39.  34. 107.  58.  37. 133.  40.  39.  47.  40.  40.  44.\n",
      "  68.  75.  88.  43.  79.  45.  54.  49.  56.  58.  42.  82.  54.  64.\n",
      "  44.  35.  55.  93.  68.  43.  37.  32.  38.  35.  83.  50.  48.  48.\n",
      "  61.  59.  51.  48.  45.  46.  50.  82.]\n",
      "58.6\n",
      "[0.60902256 0.55639098 0.         0.         0.80451128 0.\n",
      " 0.         1.         0.         0.         0.         0.\n",
      " 0.         0.         0.5112782  0.56390977 0.66165414 0.\n",
      " 0.59398496 0.         0.         0.         0.         0.\n",
      " 0.         0.61654135 0.         0.48120301 0.         0.\n",
      " 0.         0.69924812 0.5112782  0.         0.         0.\n",
      " 0.         0.         0.62406015 0.         0.         0.\n",
      " 0.45864662 0.44360902 0.         0.         0.         0.\n",
      " 0.         0.61654135]\n",
      "train\n",
      "Generation 8  fitness : 133.0\n",
      "#################################\n",
      "[127. 187. 500. 127. 135. 116. 171. 276. 129. 145. 152. 117. 189. 148.\n",
      " 129. 123. 122. 126. 173. 149. 191. 286. 125. 132. 115. 150. 123. 136.\n",
      " 143. 172. 165.  69. 131. 126. 124. 165. 156. 164. 270.  94. 143. 150.\n",
      " 276. 123. 111. 135. 129. 165.  96. 165.]\n",
      "158.96\n",
      "[0.    0.374 1.    0.    0.    0.    0.342 0.552 0.    0.    0.    0.\n",
      " 0.378 0.    0.    0.    0.    0.    0.346 0.    0.382 0.572 0.    0.\n",
      " 0.    0.    0.    0.    0.    0.344 0.33  0.    0.    0.    0.    0.33\n",
      " 0.    0.328 0.54  0.    0.    0.    0.552 0.    0.    0.    0.    0.33\n",
      " 0.    0.33 ]\n",
      "train\n",
      "Generation 9  fitness : 500.0\n",
      "#################################\n",
      "[500. 158. 128. 500. 147. 137. 146. 167. 124. 182. 129. 215. 500. 500.\n",
      " 135. 136. 144. 149. 123. 134. 456. 144. 500. 500. 141. 167. 134.  99.\n",
      " 150. 500. 152. 146. 165. 156. 169. 500. 177. 161. 139. 117.  98. 149.\n",
      " 149. 162. 500. 184. 126. 178. 128. 260.]\n",
      "249.56\n",
      "[1.    0.    0.    1.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 1.    1.    0.    0.    0.    0.    0.    0.    0.912 0.    1.    1.\n",
      " 0.    0.    0.    0.    0.    1.    0.    0.    0.    0.    0.    1.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    0.\n",
      " 0.    0.52 ]\n",
      "train\n",
      "Generation 10  fitness : 500.0\n",
      "#################################\n",
      "[146. 124. 130. 138. 500. 135. 225. 120. 111. 119. 194. 152. 120. 117.\n",
      " 138. 136. 139.  79. 127. 119. 120. 101. 500. 500. 136. 124. 149. 111.\n",
      " 226. 187. 113. 137. 500. 137. 117. 163. 500. 171. 110. 119. 132. 114.\n",
      " 117. 329. 130. 500. 141. 500. 142. 122.]\n",
      "284.62\n",
      "[0.    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    1.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    0.\n",
      " 1.    0.    0.    0.    0.    0.    0.    0.658 0.    1.    0.    1.\n",
      " 0.    0.   ]\n",
      "train\n",
      "Generation 11  fitness : 500.0\n",
      "#################################\n",
      "[ 90.  99.  71.  70.  50.  70.  87. 239.  91.  87.  60.  78.  73.  68.\n",
      "  78.  73.  60.  84.  90.  59.  99.  56.  68.  76.  75.  56.  76.  53.\n",
      "  42.  73. 170.  76.  72.  69. 110.  57.  56.  61.  61. 122.  57.  55.\n",
      "  47.  54.  93.  91.  60.  55.  69.  46.]\n",
      "195.16\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 12  fitness : 239.0\n",
      "#################################\n",
      "[222. 452. 147. 341. 500. 341. 145. 500. 251. 327. 500. 500. 500. 500.\n",
      " 500. 124. 500. 500. 500. 500. 183. 500. 500. 500. 330. 175. 308. 136.\n",
      " 500. 500. 500. 500. 500. 500. 432. 500. 500. 500. 500. 500. 155. 500.\n",
      " 279. 500. 287. 146. 500. 500. 500. 500.]\n",
      "408.6\n",
      "[0.    0.904 0.    0.    1.    0.    0.    1.    0.    0.    1.    1.\n",
      " 1.    1.    1.    0.    1.    1.    1.    1.    0.    1.    1.    1.\n",
      " 0.    0.    0.    0.    1.    1.    1.    1.    1.    1.    0.864 1.\n",
      " 1.    1.    1.    1.    0.    1.    0.    1.    0.    0.    1.    1.\n",
      " 1.    1.   ]\n"
     ]
    }
   ],
   "source": [
    "#randomly inititialise starting population\n",
    "population_size = 50\n",
    "population = []\n",
    "for p in range(population_size):\n",
    "    population.append(Creature().to(device))\n",
    "    \n",
    "gen = Generator(86).to(device)\n",
    "#gen(torch.zeros([10,86*2])).shape\n",
    "\n",
    "dis = Discriminator().to(device)\n",
    "#dis(torch.zeros([10,86])).shape\n",
    "\n",
    "lr = 0.0002\n",
    "gen_optimizer = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "dis_optimizer = torch.optim.Adam(dis.parameters(), lr=lr)\n",
    "\n",
    "print(\"starting training\")\n",
    "n_generations = 100\n",
    "batch_size = 50\n",
    "for i in range(n_generations):\n",
    "    #gen = Generator(86).to(device)\n",
    "    #dis = Discriminator().to(device)\n",
    "    gen_optimizer = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "    #dis_optimizer = torch.optim.Adam(dis.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    n_behavior_samples = 50\n",
    "    p_fitness, behavior_samples = measure_population_fitness(population,env,device,discrete_actions,\n",
    "                                                             max_steps = 500, \n",
    "                                                             n_behavior_samples=n_behavior_samples)\n",
    "    print(p_fitness)\n",
    "    if i == 0:\n",
    "        old_fitness = p_fitness\n",
    "    \n",
    "    if i == 0:\n",
    "        train_gan(population,p_fitness,old_fitness,batch_size = batch_size,n_epochs = 150)\n",
    "        #dis_optimizer = torch.optim.Adam(dis.parameters(), lr=0.00001)\n",
    "        print(\"train\")\n",
    "    else:\n",
    "        train_gan(population,p_fitness,old_fitness,batch_size = batch_size,n_epochs = 50)\n",
    "        print(\"train\")\n",
    "    old_fitness = p_fitness\n",
    "    \n",
    "    \n",
    "   # if i % 1 == 0:\n",
    "      #  fitness = measure_fitness(population[np.argmax(p_fitness)],env,device,discrete_actions,render = True)\n",
    "    population = evolve(population,gen,p_fitness,True)\n",
    "    print(\"Generation {}  fitness : {}\".format(i+1,np.max(p_fitness)))\n",
    "    print(\"#################################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeet  = np.arange(10) + 5\n",
    "print(yeet)\n",
    "yeet = np.delete(yeet,np.where(yeet<7)[0])\n",
    "print(yeet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
